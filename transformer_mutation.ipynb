{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v40VMGAE8M15"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"customized-mutants.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "k0zkUvCC8bjy",
        "outputId": "0a64cabe-cd81-4f55-bd0d-57730190b0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      projectId  bugId  methodName  mutantId     compositeId  \\\n",
              "0         Codec     18         NaN     10390  Codec-18/10390   \n",
              "1         Codec     18         NaN      6773   Codec-18/6773   \n",
              "2         Codec     18         NaN     12430  Codec-18/12430   \n",
              "3         Codec     18         NaN      6726   Codec-18/6726   \n",
              "4         Codec     18         NaN     12433  Codec-18/12433   \n",
              "...         ...    ...         ...       ...             ...   \n",
              "24818     Codec     18  write(int)      1610   Codec-18/1610   \n",
              "24819     Codec     18  write(int)      1615   Codec-18/1615   \n",
              "24820     Codec     18  write(int)      1614   Codec-18/1614   \n",
              "24821     Codec     18  write(int)      1612   Codec-18/1612   \n",
              "24822     Codec     18  write(int)      1611   Codec-18/1611   \n",
              "\n",
              "                                               className  lineNumber  \\\n",
              "0         org.apache.commons.codec.digest.PureJavaCrc32C         345   \n",
              "1          org.apache.commons.codec.digest.PureJavaCrc32         438   \n",
              "2         org.apache.commons.codec.digest.PureJavaCrc32C         604   \n",
              "3          org.apache.commons.codec.digest.PureJavaCrc32         431   \n",
              "4         org.apache.commons.codec.digest.PureJavaCrc32C         604   \n",
              "...                                                  ...         ...   \n",
              "24818  org.apache.commons.codec.binary.BaseNCodecOutp...          67   \n",
              "24819  org.apache.commons.codec.binary.BaseNCodecOutp...          68   \n",
              "24820  org.apache.commons.codec.binary.BaseNCodecOutp...          68   \n",
              "24821  org.apache.commons.codec.binary.BaseNCodecOutp...          68   \n",
              "24822  org.apache.commons.codec.binary.BaseNCodecOutp...          67   \n",
              "\n",
              "                                           testSignature  \\\n",
              "0      AvEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n",
              "1      AvEAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n",
              "2      AvEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n",
              "3      AvEAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n",
              "4      AvEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAA...   \n",
              "...                                                  ...   \n",
              "24818  AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...   \n",
              "24819  AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...   \n",
              "24820  AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...   \n",
              "24821  AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...   \n",
              "24822  AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...   \n",
              "\n",
              "      mutationOperatorGroup         mutationOperator  ... nestingLoop  \\\n",
              "0                       LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "1                       LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "2                       LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "3                       LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "4                       LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "...                     ...                      ...  ...         ...   \n",
              "24818                   LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "24819                   LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "24820                   LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "24821                   LVR  INT_LITERAL:INT_LITERAL  ...           1   \n",
              "24822                   STD         <ASSIGN>:<NO-OP>  ...           1   \n",
              "\n",
              "      nestingIf maxNestingInSameMethod nestingRatioTotal nestingRatioLoop  \\\n",
              "0             1                      5               0.2              0.2   \n",
              "1             1                      5               0.2              0.2   \n",
              "2             1                      5               0.2              0.2   \n",
              "3             1                      5               0.2              0.2   \n",
              "4             1                      5               0.2              0.2   \n",
              "...         ...                    ...               ...              ...   \n",
              "24818         1                      1               1.0              1.0   \n",
              "24819         1                      1               1.0              1.0   \n",
              "24820         1                      1               1.0              1.0   \n",
              "24821         1                      1               1.0              1.0   \n",
              "24822         1                      1               1.0              1.0   \n",
              "\n",
              "      nestingRatioIf numMutantsInSameMethod maxLineNumberInSameMethod  \\\n",
              "0                0.2                  11584                       939   \n",
              "1                0.2                  11584                       939   \n",
              "2                0.2                  11584                       939   \n",
              "3                0.2                  11584                       939   \n",
              "4                0.2                  11584                       939   \n",
              "...              ...                    ...                       ...   \n",
              "24818            1.0                      8                        68   \n",
              "24819            1.0                      8                        68   \n",
              "24820            1.0                      8                        68   \n",
              "24821            1.0                      8                        68   \n",
              "24822            1.0                      8                        68   \n",
              "\n",
              "      minLineNumberInSameMethod lineRatio  \n",
              "0                            29  0.347253  \n",
              "1                            29  0.449451  \n",
              "2                            29  0.631868  \n",
              "3                            29  0.441758  \n",
              "4                            29  0.631868  \n",
              "...                         ...       ...  \n",
              "24818                        67  0.000000  \n",
              "24819                        67  1.000000  \n",
              "24820                        67  1.000000  \n",
              "24821                        67  1.000000  \n",
              "24822                        67  0.000000  \n",
              "\n",
              "[24823 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e752e926-ed55-486a-9f92-21007a952fc8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>projectId</th>\n",
              "      <th>bugId</th>\n",
              "      <th>methodName</th>\n",
              "      <th>mutantId</th>\n",
              "      <th>compositeId</th>\n",
              "      <th>className</th>\n",
              "      <th>lineNumber</th>\n",
              "      <th>testSignature</th>\n",
              "      <th>mutationOperatorGroup</th>\n",
              "      <th>mutationOperator</th>\n",
              "      <th>...</th>\n",
              "      <th>nestingLoop</th>\n",
              "      <th>nestingIf</th>\n",
              "      <th>maxNestingInSameMethod</th>\n",
              "      <th>nestingRatioTotal</th>\n",
              "      <th>nestingRatioLoop</th>\n",
              "      <th>nestingRatioIf</th>\n",
              "      <th>numMutantsInSameMethod</th>\n",
              "      <th>maxLineNumberInSameMethod</th>\n",
              "      <th>minLineNumberInSameMethod</th>\n",
              "      <th>lineRatio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10390</td>\n",
              "      <td>Codec-18/10390</td>\n",
              "      <td>org.apache.commons.codec.digest.PureJavaCrc32C</td>\n",
              "      <td>345</td>\n",
              "      <td>AvEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>11584</td>\n",
              "      <td>939</td>\n",
              "      <td>29</td>\n",
              "      <td>0.347253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6773</td>\n",
              "      <td>Codec-18/6773</td>\n",
              "      <td>org.apache.commons.codec.digest.PureJavaCrc32</td>\n",
              "      <td>438</td>\n",
              "      <td>AvEAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>11584</td>\n",
              "      <td>939</td>\n",
              "      <td>29</td>\n",
              "      <td>0.449451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12430</td>\n",
              "      <td>Codec-18/12430</td>\n",
              "      <td>org.apache.commons.codec.digest.PureJavaCrc32C</td>\n",
              "      <td>604</td>\n",
              "      <td>AvEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>11584</td>\n",
              "      <td>939</td>\n",
              "      <td>29</td>\n",
              "      <td>0.631868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6726</td>\n",
              "      <td>Codec-18/6726</td>\n",
              "      <td>org.apache.commons.codec.digest.PureJavaCrc32</td>\n",
              "      <td>431</td>\n",
              "      <td>AvEAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>11584</td>\n",
              "      <td>939</td>\n",
              "      <td>29</td>\n",
              "      <td>0.441758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12433</td>\n",
              "      <td>Codec-18/12433</td>\n",
              "      <td>org.apache.commons.codec.digest.PureJavaCrc32C</td>\n",
              "      <td>604</td>\n",
              "      <td>AvEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>11584</td>\n",
              "      <td>939</td>\n",
              "      <td>29</td>\n",
              "      <td>0.631868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24818</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>write(int)</td>\n",
              "      <td>1610</td>\n",
              "      <td>Codec-18/1610</td>\n",
              "      <td>org.apache.commons.codec.binary.BaseNCodecOutp...</td>\n",
              "      <td>67</td>\n",
              "      <td>AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8</td>\n",
              "      <td>68</td>\n",
              "      <td>67</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24819</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>write(int)</td>\n",
              "      <td>1615</td>\n",
              "      <td>Codec-18/1615</td>\n",
              "      <td>org.apache.commons.codec.binary.BaseNCodecOutp...</td>\n",
              "      <td>68</td>\n",
              "      <td>AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8</td>\n",
              "      <td>68</td>\n",
              "      <td>67</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24820</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>write(int)</td>\n",
              "      <td>1614</td>\n",
              "      <td>Codec-18/1614</td>\n",
              "      <td>org.apache.commons.codec.binary.BaseNCodecOutp...</td>\n",
              "      <td>68</td>\n",
              "      <td>AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8</td>\n",
              "      <td>68</td>\n",
              "      <td>67</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24821</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>write(int)</td>\n",
              "      <td>1612</td>\n",
              "      <td>Codec-18/1612</td>\n",
              "      <td>org.apache.commons.codec.binary.BaseNCodecOutp...</td>\n",
              "      <td>68</td>\n",
              "      <td>AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>LVR</td>\n",
              "      <td>INT_LITERAL:INT_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8</td>\n",
              "      <td>68</td>\n",
              "      <td>67</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24822</th>\n",
              "      <td>Codec</td>\n",
              "      <td>18</td>\n",
              "      <td>write(int)</td>\n",
              "      <td>1611</td>\n",
              "      <td>Codec-18/1611</td>\n",
              "      <td>org.apache.commons.codec.binary.BaseNCodecOutp...</td>\n",
              "      <td>67</td>\n",
              "      <td>AvEAAAAAACAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>STD</td>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8</td>\n",
              "      <td>68</td>\n",
              "      <td>67</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24823 rows × 48 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e752e926-ed55-486a-9f92-21007a952fc8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e752e926-ed55-486a-9f92-21007a952fc8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e752e926-ed55-486a-9f92-21007a952fc8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"pKillsDom\"] >= 0.75] # 0.25, 0.5, 0.75"
      ],
      "metadata": {
        "id": "AmvoLlZF8cK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_list = df.to_dict('records')\n",
        "before_mutations = []\n",
        "after_mutations = []\n",
        "for dd in dict_list:\n",
        "  mutationOperator = dd[\"mutationOperator\"]\n",
        "  ml = mutationOperator.split(\":\")\n",
        "  before_mutations.append(ml[0])\n",
        "  after_mutations .append(ml[1])\n",
        "df[\"before_mutation\"] = before_mutations\n",
        "df[\"after_mutation\"] = after_mutations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN4_l2Gb8gUj",
        "outputId": "47c8821f-bc1a-47c3-e537-69c939e3c599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-111-67678cebeffa>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"before_mutation\"] = before_mutations\n",
            "<ipython-input-111-67678cebeffa>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"after_mutation\"] = after_mutations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "dict_list = df.to_dict('records')\n",
        "for dd in dict_list:\n",
        "  qa_pairs = []\n",
        "  if dd[\"methodName\"] and not pd.isna(dd[\"methodName\"]):\n",
        "    methodName = dd[\"methodName\"] \n",
        "  else:\n",
        "    methodName = \"Unknown\"\n",
        "  operation_before = dd[\"before_mutation\"]\n",
        "  mutationOperatorGroup = dd[\"mutationOperatorGroup\"]\n",
        "  operation_after = dd[\"after_mutation\"]\n",
        "  nestingLoop = dd[\"nestingLoop\"]\n",
        "  nestingIf = dd[\"nestingIf\"]\n",
        "  question = [methodName, operation_before, f\"nestingLoop\", f\"nestingIf\"]\n",
        "  answer = [dd[\"after_mutation\"]]\n",
        "  qa_pairs.append(question)\n",
        "  qa_pairs.append(answer)\n",
        "  pairs.append(qa_pairs)"
      ],
      "metadata": {
        "id": "XhBy4UbU8jju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs[0][0])\n",
        "print(pairs[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPokP0wF_eSH",
        "outputId": "7714d421-c022-4f44-b714-6fe08b08879c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Unknown', 'INT_LITERAL', 'nestingLoop', 'nestingIf']\n",
            "['INT_LITERAL']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = Counter()\n",
        "for pair in pairs:\n",
        "    word_freq.update(pair[0])\n",
        "    word_freq.update(pair[1])"
      ],
      "metadata": {
        "id": "O4QuBpve9D15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_word_freq = 5\n",
        "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
        "word_map['<unk>'] = len(word_map) + 1\n",
        "word_map['<start>'] = len(word_map) + 1\n",
        "word_map['<end>'] = len(word_map) + 1\n",
        "word_map['<pad>'] = 0"
      ],
      "metadata": {
        "id": "Hg6u5iVp9F27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total words are: {}\".format(len(word_map)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhcAlNo49H7W",
        "outputId": "4b1235c9-6005-4063-903e-400814b55f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words are: 176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 30"
      ],
      "metadata": {
        "id": "FQhaHuj4_yXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_question(words, word_map):\n",
        "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (MAX_LEN - len(words))\n",
        "    return enc_c"
      ],
      "metadata": {
        "id": "0NRpFh0Q9J-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_reply(words, word_map):\n",
        "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
        "    [word_map['<end>']] + [word_map['<pad>']] * (MAX_LEN - len(words))\n",
        "    return enc_c"
      ],
      "metadata": {
        "id": "kpnKuHwc9L6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_encoded = []\n",
        "for pair in pairs:\n",
        "    qus = encode_question(pair[0], word_map)\n",
        "    ans = encode_reply(pair[1], word_map)\n",
        "    pairs_encoded.append([qus, ans])"
      ],
      "metadata": {
        "id": "GgUWOliO9Nz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pairs = pairs_encoded\n",
        "        self.dataset_size = len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        question = torch.LongTensor(self.pairs[i][0])\n",
        "        reply = torch.LongTensor(self.pairs[i][1])\n",
        "        return question, reply\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "metadata": {
        "id": "vMxJHj5V9QJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
        "                                           batch_size = 64, \n",
        "                                           shuffle=True, \n",
        "                                           pin_memory=True)"
      ],
      "metadata": {
        "id": "yOPQm1ly9R_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(question, reply_input, reply_target):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask, reply_target_mask"
      ],
      "metadata": {
        "id": "MobYqfLk9UAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = 50):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding"
      ],
      "metadata": {
        "id": "KD3K7hox9WJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted "
      ],
      "metadata": {
        "id": "lAosbXNK9Ybi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "ssAC5cHf9aN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "iYQFb0Ci9bzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "m25hhHcm9dWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads, num_layers, word_map):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = len(word_map)\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1rZBCMGk9e4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()    "
      ],
      "metadata": {
        "id": "ljMvft_i9gc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "C1j3H8tY9iDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "heads = 4\n",
        "num_layers = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 100\n",
        "\n",
        "# with open('WORDMAP_corpus.json', 'r') as j:\n",
        "#     word_map = json.load(j)\n",
        "    \n",
        "transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map)\n",
        "transformer = transformer.to(device)\n",
        "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "criterion = LossWithLS(len(word_map), 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Z5RAb99j1p",
        "outputId": "42c8dfcc-7dc7-4f40-efbb-1307d04f58f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, transformer, criterion, epoch):\n",
        "    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for i, (question, reply) in enumerate(train_loader):\n",
        "        \n",
        "        samples = question.shape[0]\n",
        "\n",
        "        # Move to device\n",
        "        question = question.to(device)\n",
        "        reply = reply.to(device)\n",
        "\n",
        "        # Prepare Target Data\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
        "\n",
        "        # Get the transformer outputs\n",
        "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(out, reply_target, reply_target_mask)\n",
        "        \n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n",
        "        \n",
        "        sum_loss += loss.item() * samples\n",
        "        count += samples\n",
        "        \n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"
      ],
      "metadata": {
        "id": "XIlHw1fG9lh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
        "    \"\"\"\n",
        "    Performs Greedy Decoding with a batch size of 1\n",
        "    \"\"\"\n",
        "    rev_word_map = {v: k for k, v in word_map.items()}\n",
        "    transformer.eval()\n",
        "    start_token = word_map['<start>']\n",
        "    encoded = transformer.encode(question, question_mask)\n",
        "    words = torch.LongTensor([[start_token]]).to(device)\n",
        "    \n",
        "    for step in range(max_len - 1):\n",
        "        size = words.shape[1]\n",
        "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
        "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
        "        predictions = transformer.logit(decoded[:, -1])\n",
        "        _, next_word = torch.max(predictions, dim = 1)\n",
        "        next_word = next_word.item()\n",
        "        if next_word == word_map['<end>']:\n",
        "            break\n",
        "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
        "        \n",
        "    # Construct Sentence\n",
        "    if words.dim() == 2:\n",
        "        words = words.squeeze(0)\n",
        "        words = words.tolist()\n",
        "        \n",
        "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
        "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "1UXjOIU79nWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    \n",
        "    train(train_loader, transformer, criterion, epoch)\n",
        "    \n",
        "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
        "    # torch.save(state, 'checkpoint_' + str(epoch) + '.pth.tar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9JQprZx9o-6",
        "outputId": "271b1010-3ccb-423a-d850-797cc0975fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0][0/129]\tLoss: 4.548\n",
            "Epoch [0][100/129]\tLoss: 1.903\n",
            "Epoch [1][0/129]\tLoss: 0.513\n",
            "Epoch [1][100/129]\tLoss: 0.424\n",
            "Epoch [2][0/129]\tLoss: 0.247\n",
            "Epoch [2][100/129]\tLoss: 0.270\n",
            "Epoch [3][0/129]\tLoss: 0.259\n",
            "Epoch [3][100/129]\tLoss: 0.254\n",
            "Epoch [4][0/129]\tLoss: 0.247\n",
            "Epoch [4][100/129]\tLoss: 0.250\n",
            "Epoch [5][0/129]\tLoss: 0.230\n",
            "Epoch [5][100/129]\tLoss: 0.245\n",
            "Epoch [6][0/129]\tLoss: 0.262\n",
            "Epoch [6][100/129]\tLoss: 0.245\n",
            "Epoch [7][0/129]\tLoss: 0.182\n",
            "Epoch [7][100/129]\tLoss: 0.241\n",
            "Epoch [8][0/129]\tLoss: 0.261\n",
            "Epoch [8][100/129]\tLoss: 0.241\n",
            "Epoch [9][0/129]\tLoss: 0.190\n",
            "Epoch [9][100/129]\tLoss: 0.241\n",
            "Epoch [10][0/129]\tLoss: 0.241\n",
            "Epoch [10][100/129]\tLoss: 0.238\n",
            "Epoch [11][0/129]\tLoss: 0.181\n",
            "Epoch [11][100/129]\tLoss: 0.237\n",
            "Epoch [12][0/129]\tLoss: 0.202\n",
            "Epoch [12][100/129]\tLoss: 0.236\n",
            "Epoch [13][0/129]\tLoss: 0.212\n",
            "Epoch [13][100/129]\tLoss: 0.234\n",
            "Epoch [14][0/129]\tLoss: 0.164\n",
            "Epoch [14][100/129]\tLoss: 0.236\n",
            "Epoch [15][0/129]\tLoss: 0.239\n",
            "Epoch [15][100/129]\tLoss: 0.236\n",
            "Epoch [16][0/129]\tLoss: 0.257\n",
            "Epoch [16][100/129]\tLoss: 0.234\n",
            "Epoch [17][0/129]\tLoss: 0.249\n",
            "Epoch [17][100/129]\tLoss: 0.238\n",
            "Epoch [18][0/129]\tLoss: 0.237\n",
            "Epoch [18][100/129]\tLoss: 0.248\n",
            "Epoch [19][0/129]\tLoss: 0.439\n",
            "Epoch [19][100/129]\tLoss: 0.247\n",
            "Epoch [20][0/129]\tLoss: 0.268\n",
            "Epoch [20][100/129]\tLoss: 0.235\n",
            "Epoch [21][0/129]\tLoss: 0.277\n",
            "Epoch [21][100/129]\tLoss: 0.238\n",
            "Epoch [22][0/129]\tLoss: 0.315\n",
            "Epoch [22][100/129]\tLoss: 0.280\n",
            "Epoch [23][0/129]\tLoss: 0.216\n",
            "Epoch [23][100/129]\tLoss: 0.306\n",
            "Epoch [24][0/129]\tLoss: 0.186\n",
            "Epoch [24][100/129]\tLoss: 0.242\n",
            "Epoch [25][0/129]\tLoss: 0.245\n",
            "Epoch [25][100/129]\tLoss: 0.245\n",
            "Epoch [26][0/129]\tLoss: 0.286\n",
            "Epoch [26][100/129]\tLoss: 0.248\n",
            "Epoch [27][0/129]\tLoss: 0.218\n",
            "Epoch [27][100/129]\tLoss: 0.249\n",
            "Epoch [28][0/129]\tLoss: 0.196\n",
            "Epoch [28][100/129]\tLoss: 0.499\n",
            "Epoch [29][0/129]\tLoss: 0.500\n",
            "Epoch [29][100/129]\tLoss: 0.612\n",
            "Epoch [30][0/129]\tLoss: 0.653\n",
            "Epoch [30][100/129]\tLoss: 0.426\n",
            "Epoch [31][0/129]\tLoss: 0.235\n",
            "Epoch [31][100/129]\tLoss: 0.386\n",
            "Epoch [32][0/129]\tLoss: 0.287\n",
            "Epoch [32][100/129]\tLoss: 0.331\n",
            "Epoch [33][0/129]\tLoss: 0.236\n",
            "Epoch [33][100/129]\tLoss: 0.285\n",
            "Epoch [34][0/129]\tLoss: 0.372\n",
            "Epoch [34][100/129]\tLoss: 0.273\n",
            "Epoch [35][0/129]\tLoss: 0.930\n",
            "Epoch [35][100/129]\tLoss: 0.292\n",
            "Epoch [36][0/129]\tLoss: 0.190\n",
            "Epoch [36][100/129]\tLoss: 0.279\n",
            "Epoch [37][0/129]\tLoss: 0.279\n",
            "Epoch [37][100/129]\tLoss: 0.302\n",
            "Epoch [38][0/129]\tLoss: 0.225\n",
            "Epoch [38][100/129]\tLoss: 0.284\n",
            "Epoch [39][0/129]\tLoss: 0.263\n",
            "Epoch [39][100/129]\tLoss: 0.262\n",
            "Epoch [40][0/129]\tLoss: 0.204\n",
            "Epoch [40][100/129]\tLoss: 0.289\n",
            "Epoch [41][0/129]\tLoss: 0.243\n",
            "Epoch [41][100/129]\tLoss: 0.267\n",
            "Epoch [42][0/129]\tLoss: 0.307\n",
            "Epoch [42][100/129]\tLoss: 0.263\n",
            "Epoch [43][0/129]\tLoss: 0.227\n",
            "Epoch [43][100/129]\tLoss: 0.256\n",
            "Epoch [44][0/129]\tLoss: 0.283\n",
            "Epoch [44][100/129]\tLoss: 0.290\n",
            "Epoch [45][0/129]\tLoss: 0.195\n",
            "Epoch [45][100/129]\tLoss: 0.257\n",
            "Epoch [46][0/129]\tLoss: 0.289\n",
            "Epoch [46][100/129]\tLoss: 0.246\n",
            "Epoch [47][0/129]\tLoss: 0.232\n",
            "Epoch [47][100/129]\tLoss: 0.255\n",
            "Epoch [48][0/129]\tLoss: 0.298\n",
            "Epoch [48][100/129]\tLoss: 0.255\n",
            "Epoch [49][0/129]\tLoss: 0.346\n",
            "Epoch [49][100/129]\tLoss: 0.253\n",
            "Epoch [50][0/129]\tLoss: 0.261\n",
            "Epoch [50][100/129]\tLoss: 0.254\n",
            "Epoch [51][0/129]\tLoss: 0.298\n",
            "Epoch [51][100/129]\tLoss: 0.254\n",
            "Epoch [52][0/129]\tLoss: 0.252\n",
            "Epoch [52][100/129]\tLoss: 0.245\n",
            "Epoch [53][0/129]\tLoss: 0.269\n",
            "Epoch [53][100/129]\tLoss: 0.253\n",
            "Epoch [54][0/129]\tLoss: 0.261\n",
            "Epoch [54][100/129]\tLoss: 0.257\n",
            "Epoch [55][0/129]\tLoss: 0.276\n",
            "Epoch [55][100/129]\tLoss: 0.247\n",
            "Epoch [56][0/129]\tLoss: 0.293\n",
            "Epoch [56][100/129]\tLoss: 0.247\n",
            "Epoch [57][0/129]\tLoss: 0.275\n",
            "Epoch [57][100/129]\tLoss: 0.244\n",
            "Epoch [58][0/129]\tLoss: 0.181\n",
            "Epoch [58][100/129]\tLoss: 0.239\n",
            "Epoch [59][0/129]\tLoss: 0.281\n",
            "Epoch [59][100/129]\tLoss: 0.250\n",
            "Epoch [60][0/129]\tLoss: 0.255\n",
            "Epoch [60][100/129]\tLoss: 0.253\n",
            "Epoch [61][0/129]\tLoss: 0.165\n",
            "Epoch [61][100/129]\tLoss: 0.241\n",
            "Epoch [62][0/129]\tLoss: 0.193\n",
            "Epoch [62][100/129]\tLoss: 0.249\n",
            "Epoch [63][0/129]\tLoss: 0.216\n",
            "Epoch [63][100/129]\tLoss: 0.243\n",
            "Epoch [64][0/129]\tLoss: 0.293\n",
            "Epoch [64][100/129]\tLoss: 0.241\n",
            "Epoch [65][0/129]\tLoss: 0.248\n",
            "Epoch [65][100/129]\tLoss: 0.245\n",
            "Epoch [66][0/129]\tLoss: 0.229\n",
            "Epoch [66][100/129]\tLoss: 0.243\n",
            "Epoch [67][0/129]\tLoss: 0.247\n",
            "Epoch [67][100/129]\tLoss: 0.238\n",
            "Epoch [68][0/129]\tLoss: 0.196\n",
            "Epoch [68][100/129]\tLoss: 0.243\n",
            "Epoch [69][0/129]\tLoss: 0.219\n",
            "Epoch [69][100/129]\tLoss: 0.240\n",
            "Epoch [70][0/129]\tLoss: 0.214\n",
            "Epoch [70][100/129]\tLoss: 0.240\n",
            "Epoch [71][0/129]\tLoss: 0.229\n",
            "Epoch [71][100/129]\tLoss: 0.244\n",
            "Epoch [72][0/129]\tLoss: 0.235\n",
            "Epoch [72][100/129]\tLoss: 0.242\n",
            "Epoch [73][0/129]\tLoss: 0.246\n",
            "Epoch [73][100/129]\tLoss: 0.238\n",
            "Epoch [74][0/129]\tLoss: 0.269\n",
            "Epoch [74][100/129]\tLoss: 0.246\n",
            "Epoch [75][0/129]\tLoss: 0.160\n",
            "Epoch [75][100/129]\tLoss: 0.242\n",
            "Epoch [76][0/129]\tLoss: 0.254\n",
            "Epoch [76][100/129]\tLoss: 0.244\n",
            "Epoch [77][0/129]\tLoss: 0.266\n",
            "Epoch [77][100/129]\tLoss: 0.241\n",
            "Epoch [78][0/129]\tLoss: 0.216\n",
            "Epoch [78][100/129]\tLoss: 0.240\n",
            "Epoch [79][0/129]\tLoss: 0.229\n",
            "Epoch [79][100/129]\tLoss: 0.241\n",
            "Epoch [80][0/129]\tLoss: 0.205\n",
            "Epoch [80][100/129]\tLoss: 0.241\n",
            "Epoch [81][0/129]\tLoss: 0.219\n",
            "Epoch [81][100/129]\tLoss: 0.237\n",
            "Epoch [82][0/129]\tLoss: 0.290\n",
            "Epoch [82][100/129]\tLoss: 0.239\n",
            "Epoch [83][0/129]\tLoss: 0.229\n",
            "Epoch [83][100/129]\tLoss: 0.242\n",
            "Epoch [84][0/129]\tLoss: 0.194\n",
            "Epoch [84][100/129]\tLoss: 0.244\n",
            "Epoch [85][0/129]\tLoss: 0.235\n",
            "Epoch [85][100/129]\tLoss: 0.239\n",
            "Epoch [86][0/129]\tLoss: 0.259\n",
            "Epoch [86][100/129]\tLoss: 0.239\n",
            "Epoch [87][0/129]\tLoss: 0.200\n",
            "Epoch [87][100/129]\tLoss: 0.239\n",
            "Epoch [88][0/129]\tLoss: 0.325\n",
            "Epoch [88][100/129]\tLoss: 0.238\n",
            "Epoch [89][0/129]\tLoss: 0.238\n",
            "Epoch [89][100/129]\tLoss: 0.232\n",
            "Epoch [90][0/129]\tLoss: 0.238\n",
            "Epoch [90][100/129]\tLoss: 0.239\n",
            "Epoch [91][0/129]\tLoss: 0.264\n",
            "Epoch [91][100/129]\tLoss: 0.238\n",
            "Epoch [92][0/129]\tLoss: 0.258\n",
            "Epoch [92][100/129]\tLoss: 0.243\n",
            "Epoch [93][0/129]\tLoss: 0.266\n",
            "Epoch [93][100/129]\tLoss: 0.263\n",
            "Epoch [94][0/129]\tLoss: 0.287\n",
            "Epoch [94][100/129]\tLoss: 0.246\n",
            "Epoch [95][0/129]\tLoss: 0.173\n",
            "Epoch [95][100/129]\tLoss: 0.237\n",
            "Epoch [96][0/129]\tLoss: 0.216\n",
            "Epoch [96][100/129]\tLoss: 0.241\n",
            "Epoch [97][0/129]\tLoss: 0.212\n",
            "Epoch [97][100/129]\tLoss: 0.239\n",
            "Epoch [98][0/129]\tLoss: 0.239\n",
            "Epoch [98][100/129]\tLoss: 0.238\n",
            "Epoch [99][0/129]\tLoss: 0.217\n",
            "Epoch [99][100/129]\tLoss: 0.236\n"
          ]
        }
      ]
    }
  ]
}