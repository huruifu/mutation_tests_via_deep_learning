{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhzoKNEg9w84"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"customized-mutants.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "IWMECOaP95ia",
        "outputId": "3c1613bd-8dae-46b7-ce63-4b9d30e6fad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     projectId  bugId                  methodName  mutantId  compositeId  \\\n",
              "0          Cli     40                         NaN      1627  Cli-40/1627   \n",
              "1          Cli     40                         NaN      2361  Cli-40/2361   \n",
              "2          Cli     40                         NaN        61    Cli-40/61   \n",
              "3          Cli     40                         NaN         8     Cli-40/8   \n",
              "4          Cli     40                         NaN       651   Cli-40/651   \n",
              "...        ...    ...                         ...       ...          ...   \n",
              "2884       Cli     40  withType(java.lang.Object)      2282  Cli-40/2282   \n",
              "2885       Cli     40        withValueSeparator()      2261  Cli-40/2261   \n",
              "2886       Cli     40        withValueSeparator()      2260  Cli-40/2260   \n",
              "2887       Cli     40    withValueSeparator(char)      2259  Cli-40/2259   \n",
              "2888       Cli     40    withValueSeparator(char)      2258  Cli-40/2258   \n",
              "\n",
              "                                        className  lineNumber  \\\n",
              "0            org.apache.commons.cli.HelpFormatter         167   \n",
              "1     org.apache.commons.cli.PatternOptionBuilder          70   \n",
              "2                   org.apache.commons.cli.Option          76   \n",
              "3              org.apache.commons.cli.OptionGroup          35   \n",
              "4                  org.apache.commons.cli.Options          47   \n",
              "...                                           ...         ...   \n",
              "2884         org.apache.commons.cli.OptionBuilder         297   \n",
              "2885         org.apache.commons.cli.OptionBuilder         200   \n",
              "2886         org.apache.commons.cli.OptionBuilder         198   \n",
              "2887         org.apache.commons.cli.OptionBuilder         177   \n",
              "2888         org.apache.commons.cli.OptionBuilder         175   \n",
              "\n",
              "                                          testSignature mutationOperatorGroup  \\\n",
              "0     AWMAAAAAAAAAAABAAAAAAAAAAAAAAAAgAAAAAEACAACBFQ...                   EVR   \n",
              "1     AWMAAAAAAAAAAAAAAAgAAAAAAAAAAAAAAAAACAAAAAAAAA...                   EVR   \n",
              "2     AWP///////////8D/vz////////////H///v/weA8Qc+iH...                   EVR   \n",
              "3     AWMABQAAAAAAFACA/gAQkAAAAAAAEARJAAYAAAAAAAAAgc...                   EVR   \n",
              "4     AWP//////////////////////////////////1cG8Ae/9/...                   EVR   \n",
              "...                                                 ...                   ...   \n",
              "2884  AWMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...                   EVR   \n",
              "2885  AWMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...                   EVR   \n",
              "2886  AWMAACAAAAAAAMAAAAAAAAACAAACACAACAAAAQAAAAAAAA...                   STD   \n",
              "2887  AWMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...                   EVR   \n",
              "2888  AWMAAAAAAAAAAAAAAAAAAAAAAAACAgAAAAAAQAIAAAAYAA...                   STD   \n",
              "\n",
              "                    mutationOperator  ... nestingLoop nestingIf  \\\n",
              "0            IDENTIFIER:NULL_LITERAL  ...           1         1   \n",
              "1         MEMBER_SELECT:NULL_LITERAL  ...           1         1   \n",
              "2             NEW_CLASS:NULL_LITERAL  ...           1         1   \n",
              "3             NEW_CLASS:NULL_LITERAL  ...           1         1   \n",
              "4             NEW_CLASS:NULL_LITERAL  ...           1         1   \n",
              "...                              ...  ...         ...       ...   \n",
              "2884  METHOD_INVOCATION:NULL_LITERAL  ...           1         1   \n",
              "2885         IDENTIFIER:NULL_LITERAL  ...           1         1   \n",
              "2886                <ASSIGN>:<NO-OP>  ...           1         1   \n",
              "2887         IDENTIFIER:NULL_LITERAL  ...           1         1   \n",
              "2888                <ASSIGN>:<NO-OP>  ...           1         1   \n",
              "\n",
              "     maxNestingInSameMethod nestingRatioTotal nestingRatioLoop nestingRatioIf  \\\n",
              "0                         1               1.0              1.0            1.0   \n",
              "1                         1               1.0              1.0            1.0   \n",
              "2                         1               1.0              1.0            1.0   \n",
              "3                         1               1.0              1.0            1.0   \n",
              "4                         1               1.0              1.0            1.0   \n",
              "...                     ...               ...              ...            ...   \n",
              "2884                      1               1.0              1.0            1.0   \n",
              "2885                      1               1.0              1.0            1.0   \n",
              "2886                      1               1.0              1.0            1.0   \n",
              "2887                      1               1.0              1.0            1.0   \n",
              "2888                      1               1.0              1.0            1.0   \n",
              "\n",
              "     numMutantsInSameMethod maxLineNumberInSameMethod  \\\n",
              "0                        36                       813   \n",
              "1                        36                       813   \n",
              "2                        36                       813   \n",
              "3                        36                       813   \n",
              "4                        36                       813   \n",
              "...                     ...                       ...   \n",
              "2884                      1                       297   \n",
              "2885                      2                       200   \n",
              "2886                      2                       200   \n",
              "2887                      2                       177   \n",
              "2888                      2                       177   \n",
              "\n",
              "     minLineNumberInSameMethod lineRatio  \n",
              "0                           35  0.169666  \n",
              "1                           35  0.044987  \n",
              "2                           35  0.052699  \n",
              "3                           35  0.000000  \n",
              "4                           35  0.015424  \n",
              "...                        ...       ...  \n",
              "2884                       297       NaN  \n",
              "2885                       198  1.000000  \n",
              "2886                       198  0.000000  \n",
              "2887                       175  1.000000  \n",
              "2888                       175  0.000000  \n",
              "\n",
              "[2889 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c84ac76-92bf-47a1-8e70-5364142f6518\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>projectId</th>\n",
              "      <th>bugId</th>\n",
              "      <th>methodName</th>\n",
              "      <th>mutantId</th>\n",
              "      <th>compositeId</th>\n",
              "      <th>className</th>\n",
              "      <th>lineNumber</th>\n",
              "      <th>testSignature</th>\n",
              "      <th>mutationOperatorGroup</th>\n",
              "      <th>mutationOperator</th>\n",
              "      <th>...</th>\n",
              "      <th>nestingLoop</th>\n",
              "      <th>nestingIf</th>\n",
              "      <th>maxNestingInSameMethod</th>\n",
              "      <th>nestingRatioTotal</th>\n",
              "      <th>nestingRatioLoop</th>\n",
              "      <th>nestingRatioIf</th>\n",
              "      <th>numMutantsInSameMethod</th>\n",
              "      <th>maxLineNumberInSameMethod</th>\n",
              "      <th>minLineNumberInSameMethod</th>\n",
              "      <th>lineRatio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1627</td>\n",
              "      <td>Cli-40/1627</td>\n",
              "      <td>org.apache.commons.cli.HelpFormatter</td>\n",
              "      <td>167</td>\n",
              "      <td>AWMAAAAAAAAAAABAAAAAAAAAAAAAAAAgAAAAAEACAACBFQ...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>IDENTIFIER:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>813</td>\n",
              "      <td>35</td>\n",
              "      <td>0.169666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2361</td>\n",
              "      <td>Cli-40/2361</td>\n",
              "      <td>org.apache.commons.cli.PatternOptionBuilder</td>\n",
              "      <td>70</td>\n",
              "      <td>AWMAAAAAAAAAAAAAAAgAAAAAAAAAAAAAAAAACAAAAAAAAA...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>MEMBER_SELECT:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>813</td>\n",
              "      <td>35</td>\n",
              "      <td>0.044987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>61</td>\n",
              "      <td>Cli-40/61</td>\n",
              "      <td>org.apache.commons.cli.Option</td>\n",
              "      <td>76</td>\n",
              "      <td>AWP///////////8D/vz////////////H///v/weA8Qc+iH...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>NEW_CLASS:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>813</td>\n",
              "      <td>35</td>\n",
              "      <td>0.052699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>Cli-40/8</td>\n",
              "      <td>org.apache.commons.cli.OptionGroup</td>\n",
              "      <td>35</td>\n",
              "      <td>AWMABQAAAAAAFACA/gAQkAAAAAAAEARJAAYAAAAAAAAAgc...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>NEW_CLASS:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>813</td>\n",
              "      <td>35</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>NaN</td>\n",
              "      <td>651</td>\n",
              "      <td>Cli-40/651</td>\n",
              "      <td>org.apache.commons.cli.Options</td>\n",
              "      <td>47</td>\n",
              "      <td>AWP//////////////////////////////////1cG8Ae/9/...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>NEW_CLASS:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>36</td>\n",
              "      <td>813</td>\n",
              "      <td>35</td>\n",
              "      <td>0.015424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2884</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>withType(java.lang.Object)</td>\n",
              "      <td>2282</td>\n",
              "      <td>Cli-40/2282</td>\n",
              "      <td>org.apache.commons.cli.OptionBuilder</td>\n",
              "      <td>297</td>\n",
              "      <td>AWMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>METHOD_INVOCATION:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>297</td>\n",
              "      <td>297</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2885</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>withValueSeparator()</td>\n",
              "      <td>2261</td>\n",
              "      <td>Cli-40/2261</td>\n",
              "      <td>org.apache.commons.cli.OptionBuilder</td>\n",
              "      <td>200</td>\n",
              "      <td>AWMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>IDENTIFIER:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>200</td>\n",
              "      <td>198</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2886</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>withValueSeparator()</td>\n",
              "      <td>2260</td>\n",
              "      <td>Cli-40/2260</td>\n",
              "      <td>org.apache.commons.cli.OptionBuilder</td>\n",
              "      <td>198</td>\n",
              "      <td>AWMAACAAAAAAAMAAAAAAAAACAAACACAACAAAAQAAAAAAAA...</td>\n",
              "      <td>STD</td>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>200</td>\n",
              "      <td>198</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2887</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>withValueSeparator(char)</td>\n",
              "      <td>2259</td>\n",
              "      <td>Cli-40/2259</td>\n",
              "      <td>org.apache.commons.cli.OptionBuilder</td>\n",
              "      <td>177</td>\n",
              "      <td>AWMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA...</td>\n",
              "      <td>EVR</td>\n",
              "      <td>IDENTIFIER:NULL_LITERAL</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>177</td>\n",
              "      <td>175</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2888</th>\n",
              "      <td>Cli</td>\n",
              "      <td>40</td>\n",
              "      <td>withValueSeparator(char)</td>\n",
              "      <td>2258</td>\n",
              "      <td>Cli-40/2258</td>\n",
              "      <td>org.apache.commons.cli.OptionBuilder</td>\n",
              "      <td>175</td>\n",
              "      <td>AWMAAAAAAAAAAAAAAAAAAAAAAAACAgAAAAAAQAIAAAAYAA...</td>\n",
              "      <td>STD</td>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>177</td>\n",
              "      <td>175</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2889 rows × 48 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c84ac76-92bf-47a1-8e70-5364142f6518')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6c84ac76-92bf-47a1-8e70-5364142f6518 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6c84ac76-92bf-47a1-8e70-5364142f6518');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"pKillsDom\"] >= 0.75] # 0.25, 0.5, 0.75"
      ],
      "metadata": {
        "id": "bCfPbe-XyP_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_list = df.to_dict('records')\n",
        "before_mutations = []\n",
        "after_mutations = []\n",
        "for dd in dict_list:\n",
        "  mutationOperator = dd[\"mutationOperator\"]\n",
        "  ml = mutationOperator.split(\":\")\n",
        "  before_mutations.append(ml[0])\n",
        "  after_mutations .append(ml[1])\n",
        "df[\"before_mutation\"] = before_mutations\n",
        "df[\"after_mutation\"] = after_mutations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5GhVb2r-VFX",
        "outputId": "b7106b92-650e-4167-d65e-5e56eee23c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-67678cebeffa>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"before_mutation\"] = before_mutations\n",
            "<ipython-input-4-67678cebeffa>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"after_mutation\"] = after_mutations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"mutationOperator\", \"before_mutation\", \"after_mutation\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ufFkn3Ou_Fm8",
        "outputId": "efea3883-4722-4c1e-a6ae-a70f239cf234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             mutationOperator before_mutation after_mutation\n",
              "4      NEW_CLASS:NULL_LITERAL       NEW_CLASS   NULL_LITERAL\n",
              "6      IDENTIFIER:INT_LITERAL      IDENTIFIER    INT_LITERAL\n",
              "7      NEW_CLASS:NULL_LITERAL       NEW_CLASS   NULL_LITERAL\n",
              "9      NEW_CLASS:NULL_LITERAL       NEW_CLASS   NULL_LITERAL\n",
              "12     IDENTIFIER:INT_LITERAL      IDENTIFIER    INT_LITERAL\n",
              "...                       ...             ...            ...\n",
              "2874         <ASSIGN>:<NO-OP>        <ASSIGN>        <NO-OP>\n",
              "2875  IDENTIFIER:NULL_LITERAL      IDENTIFIER   NULL_LITERAL\n",
              "2880         <ASSIGN>:<NO-OP>        <ASSIGN>        <NO-OP>\n",
              "2886         <ASSIGN>:<NO-OP>        <ASSIGN>        <NO-OP>\n",
              "2888         <ASSIGN>:<NO-OP>        <ASSIGN>        <NO-OP>\n",
              "\n",
              "[1440 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2d6a0f5-4651-4116-a6a2-f1a8cbc5fa6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mutationOperator</th>\n",
              "      <th>before_mutation</th>\n",
              "      <th>after_mutation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NEW_CLASS:NULL_LITERAL</td>\n",
              "      <td>NEW_CLASS</td>\n",
              "      <td>NULL_LITERAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>IDENTIFIER:INT_LITERAL</td>\n",
              "      <td>IDENTIFIER</td>\n",
              "      <td>INT_LITERAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NEW_CLASS:NULL_LITERAL</td>\n",
              "      <td>NEW_CLASS</td>\n",
              "      <td>NULL_LITERAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NEW_CLASS:NULL_LITERAL</td>\n",
              "      <td>NEW_CLASS</td>\n",
              "      <td>NULL_LITERAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IDENTIFIER:INT_LITERAL</td>\n",
              "      <td>IDENTIFIER</td>\n",
              "      <td>INT_LITERAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2874</th>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>&lt;ASSIGN&gt;</td>\n",
              "      <td>&lt;NO-OP&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2875</th>\n",
              "      <td>IDENTIFIER:NULL_LITERAL</td>\n",
              "      <td>IDENTIFIER</td>\n",
              "      <td>NULL_LITERAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2880</th>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>&lt;ASSIGN&gt;</td>\n",
              "      <td>&lt;NO-OP&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2886</th>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>&lt;ASSIGN&gt;</td>\n",
              "      <td>&lt;NO-OP&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2888</th>\n",
              "      <td>&lt;ASSIGN&gt;:&lt;NO-OP&gt;</td>\n",
              "      <td>&lt;ASSIGN&gt;</td>\n",
              "      <td>&lt;NO-OP&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1440 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2d6a0f5-4651-4116-a6a2-f1a8cbc5fa6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2d6a0f5-4651-4116-a6a2-f1a8cbc5fa6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2d6a0f5-4651-4116-a6a2-f1a8cbc5fa6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "dict_list = df.to_dict('records')\n",
        "for dd in dict_list:\n",
        "  if dd[\"methodName\"] and not pd.isna(dd[\"methodName\"]):\n",
        "    methodName = dd[\"methodName\"] \n",
        "  else:\n",
        "    methodName = \"Unknown\"\n",
        "  operation_before = dd[\"before_mutation\"]\n",
        "  mutationOperatorGroup = dd[\"mutationOperatorGroup\"]\n",
        "  operation_after = dd[\"after_mutation\"]\n",
        "  nestingLoop = dd[\"nestingLoop\"]\n",
        "  nestingIf = dd[\"nestingIf\"]\n",
        "  question = f\"{methodName} {operation_before} {nestingLoop} {nestingIf}\"\n",
        "  answer = dd[\"after_mutation\"]\n",
        "  pairs.append((question, answer))"
      ],
      "metadata": {
        "id": "UXj3ewca_Rjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tfqfh0uyd5R",
        "outputId": "1dca9b65-e84c-4b5d-b2ce-b2650577040e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Unknown METHOD_INVOCATION 1 1', 'NULL_LITERAL')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # # Remove words below a certain count threshold\n",
        "    # def trim(self, min_count):\n",
        "    #     if self.trimmed:\n",
        "    #         return\n",
        "    #     self.trimmed = True\n",
        "\n",
        "    #     keep_words = []\n",
        "\n",
        "    #     for k, v in self.word2count.items():\n",
        "    #         if v >= min_count:\n",
        "    #             keep_words.append(k)\n",
        "\n",
        "    #     print('keep_words {} / {} = {:.4f}'.format(\n",
        "    #         len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "    #     ))\n",
        "\n",
        "    #     # Reinitialize dictionaries\n",
        "    #     self.word2index = {}\n",
        "    #     self.word2count = {}\n",
        "    #     self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "    #     self.num_words = 3 # Count default tokens\n",
        "\n",
        "    #     for word in keep_words:\n",
        "    #         self.addWord(word)"
      ],
      "metadata": {
        "id": "9Y-D-tW6xoaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_voc(pairs):\n",
        "  voc = Voc()\n",
        "  for pair in pairs:\n",
        "    voc.addSentence(pair[0])\n",
        "    voc.addSentence(pair[1])\n",
        "  print(\"Counted words:\", voc.num_words)\n",
        "  return voc, pairs\n",
        "\n",
        "voc, pairs = create_voc(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73BAG-fexqgr",
        "outputId": "f51a4396-e411-4c70-a0b1-6d3687d426ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counted words: 204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    return s"
      ],
      "metadata": {
        "id": "cnvjBzzA86Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [SOS_token] + [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRGYeWIOxsoh",
        "outputId": "80868ea0-82b7-4717-f954-3e9f1553388d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[  1,   1,   1,   1,   1],\n",
            "        [122, 198, 119,   3,  79],\n",
            "        [ 26,  35,  50,   9,   8],\n",
            "        [ 36,  36,   5,   5,  36],\n",
            "        [ 70,  41,   5,   5,  71],\n",
            "        [  2,   2,   2,   2,   2]])\n",
            "lengths: tensor([6, 6, 6, 6, 6])\n",
            "target_variable: tensor([[ 1,  1,  1,  1,  1],\n",
            "        [33, 18, 52,  6,  8],\n",
            "        [ 2,  2,  2,  2,  2]])\n",
            "mask: tensor([[True, True, True, True, True],\n",
            "        [True, True, True, True, True],\n",
            "        [True, True, True, True, True]])\n",
            "max_target_len: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "gfVqZnpVxuqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights\n",
        "        attn_energies = self.score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "metadata": {
        "id": "_N1dWXg-xwcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "n6yTRbVRxyRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "9O8OJKUtx0C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "metadata": {
        "id": "gQNhIjchx2Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "metadata": {
        "id": "kk-t5TMcx4DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length=30):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "metadata": {
        "id": "dbpl31zix58V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "metadata": {
        "id": "6bmiGdBIx71k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "model_name = 'model'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "checkpoint_iter = 50\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpOAQXIJx970",
        "outputId": "95089e70-2fec-46f2-d28f-a0fce09c58b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 500\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "save_dir = os.path.join(\"save\")\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue7DRtEex_p0",
        "outputId": "f0dfb00f-1f3a-4840-e357-90ab443a3b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 234; Percent complete: 46.8%; Average loss: 0.4401\n",
            "Iteration: 235; Percent complete: 47.0%; Average loss: 0.3515\n",
            "Iteration: 236; Percent complete: 47.2%; Average loss: 0.3226\n",
            "Iteration: 237; Percent complete: 47.4%; Average loss: 0.3222\n",
            "Iteration: 238; Percent complete: 47.6%; Average loss: 0.4128\n",
            "Iteration: 239; Percent complete: 47.8%; Average loss: 0.3305\n",
            "Iteration: 240; Percent complete: 48.0%; Average loss: 0.3475\n",
            "Iteration: 241; Percent complete: 48.2%; Average loss: 0.3611\n",
            "Iteration: 242; Percent complete: 48.4%; Average loss: 0.3657\n",
            "Iteration: 243; Percent complete: 48.6%; Average loss: 0.3638\n",
            "Iteration: 244; Percent complete: 48.8%; Average loss: 0.3183\n",
            "Iteration: 245; Percent complete: 49.0%; Average loss: 0.3359\n",
            "Iteration: 246; Percent complete: 49.2%; Average loss: 0.3341\n",
            "Iteration: 247; Percent complete: 49.4%; Average loss: 0.3823\n",
            "Iteration: 248; Percent complete: 49.6%; Average loss: 0.3999\n",
            "Iteration: 249; Percent complete: 49.8%; Average loss: 0.3680\n",
            "Iteration: 250; Percent complete: 50.0%; Average loss: 0.3237\n",
            "Iteration: 251; Percent complete: 50.2%; Average loss: 0.3690\n",
            "Iteration: 252; Percent complete: 50.4%; Average loss: 0.3357\n",
            "Iteration: 253; Percent complete: 50.6%; Average loss: 0.3312\n",
            "Iteration: 254; Percent complete: 50.8%; Average loss: 0.3766\n",
            "Iteration: 255; Percent complete: 51.0%; Average loss: 0.3585\n",
            "Iteration: 256; Percent complete: 51.2%; Average loss: 0.3718\n",
            "Iteration: 257; Percent complete: 51.4%; Average loss: 0.4043\n",
            "Iteration: 258; Percent complete: 51.6%; Average loss: 0.3314\n",
            "Iteration: 259; Percent complete: 51.8%; Average loss: 0.3619\n",
            "Iteration: 260; Percent complete: 52.0%; Average loss: 0.3359\n",
            "Iteration: 261; Percent complete: 52.2%; Average loss: 0.3747\n",
            "Iteration: 262; Percent complete: 52.4%; Average loss: 0.3805\n",
            "Iteration: 263; Percent complete: 52.6%; Average loss: 0.2866\n",
            "Iteration: 264; Percent complete: 52.8%; Average loss: 0.3779\n",
            "Iteration: 265; Percent complete: 53.0%; Average loss: 0.3053\n",
            "Iteration: 266; Percent complete: 53.2%; Average loss: 0.3585\n",
            "Iteration: 267; Percent complete: 53.4%; Average loss: 0.3517\n",
            "Iteration: 268; Percent complete: 53.6%; Average loss: 0.3412\n",
            "Iteration: 269; Percent complete: 53.8%; Average loss: 0.3826\n",
            "Iteration: 270; Percent complete: 54.0%; Average loss: 0.3090\n",
            "Iteration: 271; Percent complete: 54.2%; Average loss: 0.3819\n",
            "Iteration: 272; Percent complete: 54.4%; Average loss: 0.4100\n",
            "Iteration: 273; Percent complete: 54.6%; Average loss: 0.3143\n",
            "Iteration: 274; Percent complete: 54.8%; Average loss: 0.3997\n",
            "Iteration: 275; Percent complete: 55.0%; Average loss: 0.3963\n",
            "Iteration: 276; Percent complete: 55.2%; Average loss: 0.3649\n",
            "Iteration: 277; Percent complete: 55.4%; Average loss: 0.3791\n",
            "Iteration: 278; Percent complete: 55.6%; Average loss: 0.3194\n",
            "Iteration: 279; Percent complete: 55.8%; Average loss: 0.3003\n",
            "Iteration: 280; Percent complete: 56.0%; Average loss: 0.3840\n",
            "Iteration: 281; Percent complete: 56.2%; Average loss: 0.4199\n",
            "Iteration: 282; Percent complete: 56.4%; Average loss: 0.2946\n",
            "Iteration: 283; Percent complete: 56.6%; Average loss: 0.3556\n",
            "Iteration: 284; Percent complete: 56.8%; Average loss: 0.3321\n",
            "Iteration: 285; Percent complete: 57.0%; Average loss: 0.3304\n",
            "Iteration: 286; Percent complete: 57.2%; Average loss: 0.3794\n",
            "Iteration: 287; Percent complete: 57.4%; Average loss: 0.3799\n",
            "Iteration: 288; Percent complete: 57.6%; Average loss: 0.4199\n",
            "Iteration: 289; Percent complete: 57.8%; Average loss: 0.3327\n",
            "Iteration: 290; Percent complete: 58.0%; Average loss: 0.3755\n",
            "Iteration: 291; Percent complete: 58.2%; Average loss: 0.3291\n",
            "Iteration: 292; Percent complete: 58.4%; Average loss: 0.3307\n",
            "Iteration: 293; Percent complete: 58.6%; Average loss: 0.3508\n",
            "Iteration: 294; Percent complete: 58.8%; Average loss: 0.3473\n",
            "Iteration: 295; Percent complete: 59.0%; Average loss: 0.3266\n",
            "Iteration: 296; Percent complete: 59.2%; Average loss: 0.3723\n",
            "Iteration: 297; Percent complete: 59.4%; Average loss: 0.4010\n",
            "Iteration: 298; Percent complete: 59.6%; Average loss: 0.3496\n",
            "Iteration: 299; Percent complete: 59.8%; Average loss: 0.2996\n",
            "Iteration: 300; Percent complete: 60.0%; Average loss: 0.3431\n",
            "Iteration: 301; Percent complete: 60.2%; Average loss: 0.3436\n",
            "Iteration: 302; Percent complete: 60.4%; Average loss: 0.3517\n",
            "Iteration: 303; Percent complete: 60.6%; Average loss: 0.3813\n",
            "Iteration: 304; Percent complete: 60.8%; Average loss: 0.3426\n",
            "Iteration: 305; Percent complete: 61.0%; Average loss: 0.3961\n",
            "Iteration: 306; Percent complete: 61.2%; Average loss: 0.3677\n",
            "Iteration: 307; Percent complete: 61.4%; Average loss: 0.3669\n",
            "Iteration: 308; Percent complete: 61.6%; Average loss: 0.3421\n",
            "Iteration: 309; Percent complete: 61.8%; Average loss: 0.3195\n",
            "Iteration: 310; Percent complete: 62.0%; Average loss: 0.3315\n",
            "Iteration: 311; Percent complete: 62.2%; Average loss: 0.3241\n",
            "Iteration: 312; Percent complete: 62.4%; Average loss: 0.3349\n",
            "Iteration: 313; Percent complete: 62.6%; Average loss: 0.3304\n",
            "Iteration: 314; Percent complete: 62.8%; Average loss: 0.3831\n",
            "Iteration: 315; Percent complete: 63.0%; Average loss: 0.3151\n",
            "Iteration: 316; Percent complete: 63.2%; Average loss: 0.3906\n",
            "Iteration: 317; Percent complete: 63.4%; Average loss: 0.3235\n",
            "Iteration: 318; Percent complete: 63.6%; Average loss: 0.3309\n",
            "Iteration: 319; Percent complete: 63.8%; Average loss: 0.3890\n",
            "Iteration: 320; Percent complete: 64.0%; Average loss: 0.3996\n",
            "Iteration: 321; Percent complete: 64.2%; Average loss: 0.3220\n",
            "Iteration: 322; Percent complete: 64.4%; Average loss: 0.3481\n",
            "Iteration: 323; Percent complete: 64.6%; Average loss: 0.3601\n",
            "Iteration: 324; Percent complete: 64.8%; Average loss: 0.3640\n",
            "Iteration: 325; Percent complete: 65.0%; Average loss: 0.3332\n",
            "Iteration: 326; Percent complete: 65.2%; Average loss: 0.3592\n",
            "Iteration: 327; Percent complete: 65.4%; Average loss: 0.3701\n",
            "Iteration: 328; Percent complete: 65.6%; Average loss: 0.3780\n",
            "Iteration: 329; Percent complete: 65.8%; Average loss: 0.3415\n",
            "Iteration: 330; Percent complete: 66.0%; Average loss: 0.3784\n",
            "Iteration: 331; Percent complete: 66.2%; Average loss: 0.3692\n",
            "Iteration: 332; Percent complete: 66.4%; Average loss: 0.2907\n",
            "Iteration: 333; Percent complete: 66.6%; Average loss: 0.3978\n",
            "Iteration: 334; Percent complete: 66.8%; Average loss: 0.3473\n",
            "Iteration: 335; Percent complete: 67.0%; Average loss: 0.3967\n",
            "Iteration: 336; Percent complete: 67.2%; Average loss: 0.3532\n",
            "Iteration: 337; Percent complete: 67.4%; Average loss: 0.3750\n",
            "Iteration: 338; Percent complete: 67.6%; Average loss: 0.3605\n",
            "Iteration: 339; Percent complete: 67.8%; Average loss: 0.2984\n",
            "Iteration: 340; Percent complete: 68.0%; Average loss: 0.3475\n",
            "Iteration: 341; Percent complete: 68.2%; Average loss: 0.2730\n",
            "Iteration: 342; Percent complete: 68.4%; Average loss: 0.3901\n",
            "Iteration: 343; Percent complete: 68.6%; Average loss: 0.3330\n",
            "Iteration: 344; Percent complete: 68.8%; Average loss: 0.3408\n",
            "Iteration: 345; Percent complete: 69.0%; Average loss: 0.3301\n",
            "Iteration: 346; Percent complete: 69.2%; Average loss: 0.3731\n",
            "Iteration: 347; Percent complete: 69.4%; Average loss: 0.3900\n",
            "Iteration: 348; Percent complete: 69.6%; Average loss: 0.3658\n",
            "Iteration: 349; Percent complete: 69.8%; Average loss: 0.3127\n",
            "Iteration: 350; Percent complete: 70.0%; Average loss: 0.3578\n",
            "Iteration: 351; Percent complete: 70.2%; Average loss: 0.3221\n",
            "Iteration: 352; Percent complete: 70.4%; Average loss: 0.3692\n",
            "Iteration: 353; Percent complete: 70.6%; Average loss: 0.3914\n",
            "Iteration: 354; Percent complete: 70.8%; Average loss: 0.2486\n",
            "Iteration: 355; Percent complete: 71.0%; Average loss: 0.3741\n",
            "Iteration: 356; Percent complete: 71.2%; Average loss: 0.2850\n",
            "Iteration: 357; Percent complete: 71.4%; Average loss: 0.4003\n",
            "Iteration: 358; Percent complete: 71.6%; Average loss: 0.2848\n",
            "Iteration: 359; Percent complete: 71.8%; Average loss: 0.3161\n",
            "Iteration: 360; Percent complete: 72.0%; Average loss: 0.3783\n",
            "Iteration: 361; Percent complete: 72.2%; Average loss: 0.3580\n",
            "Iteration: 362; Percent complete: 72.4%; Average loss: 0.3653\n",
            "Iteration: 363; Percent complete: 72.6%; Average loss: 0.3299\n",
            "Iteration: 364; Percent complete: 72.8%; Average loss: 0.3236\n",
            "Iteration: 365; Percent complete: 73.0%; Average loss: 0.3934\n",
            "Iteration: 366; Percent complete: 73.2%; Average loss: 0.3641\n",
            "Iteration: 367; Percent complete: 73.4%; Average loss: 0.3861\n",
            "Iteration: 368; Percent complete: 73.6%; Average loss: 0.3110\n",
            "Iteration: 369; Percent complete: 73.8%; Average loss: 0.4430\n",
            "Iteration: 370; Percent complete: 74.0%; Average loss: 0.3751\n",
            "Iteration: 371; Percent complete: 74.2%; Average loss: 0.3419\n",
            "Iteration: 372; Percent complete: 74.4%; Average loss: 0.3358\n",
            "Iteration: 373; Percent complete: 74.6%; Average loss: 0.2844\n",
            "Iteration: 374; Percent complete: 74.8%; Average loss: 0.3488\n",
            "Iteration: 375; Percent complete: 75.0%; Average loss: 0.3993\n",
            "Iteration: 376; Percent complete: 75.2%; Average loss: 0.3995\n",
            "Iteration: 377; Percent complete: 75.4%; Average loss: 0.3909\n",
            "Iteration: 378; Percent complete: 75.6%; Average loss: 0.3267\n",
            "Iteration: 379; Percent complete: 75.8%; Average loss: 0.3924\n",
            "Iteration: 380; Percent complete: 76.0%; Average loss: 0.2844\n",
            "Iteration: 381; Percent complete: 76.2%; Average loss: 0.3216\n",
            "Iteration: 382; Percent complete: 76.4%; Average loss: 0.3389\n",
            "Iteration: 383; Percent complete: 76.6%; Average loss: 0.3237\n",
            "Iteration: 384; Percent complete: 76.8%; Average loss: 0.3343\n",
            "Iteration: 385; Percent complete: 77.0%; Average loss: 0.3216\n",
            "Iteration: 386; Percent complete: 77.2%; Average loss: 0.3726\n",
            "Iteration: 387; Percent complete: 77.4%; Average loss: 0.3032\n",
            "Iteration: 388; Percent complete: 77.6%; Average loss: 0.3293\n",
            "Iteration: 389; Percent complete: 77.8%; Average loss: 0.4376\n",
            "Iteration: 390; Percent complete: 78.0%; Average loss: 0.2983\n",
            "Iteration: 391; Percent complete: 78.2%; Average loss: 0.3295\n",
            "Iteration: 392; Percent complete: 78.4%; Average loss: 0.3462\n",
            "Iteration: 393; Percent complete: 78.6%; Average loss: 0.3648\n",
            "Iteration: 394; Percent complete: 78.8%; Average loss: 0.3034\n",
            "Iteration: 395; Percent complete: 79.0%; Average loss: 0.3038\n",
            "Iteration: 396; Percent complete: 79.2%; Average loss: 0.3765\n",
            "Iteration: 397; Percent complete: 79.4%; Average loss: 0.3486\n",
            "Iteration: 398; Percent complete: 79.6%; Average loss: 0.3491\n",
            "Iteration: 399; Percent complete: 79.8%; Average loss: 0.4153\n",
            "Iteration: 400; Percent complete: 80.0%; Average loss: 0.3471\n",
            "Iteration: 401; Percent complete: 80.2%; Average loss: 0.3683\n",
            "Iteration: 402; Percent complete: 80.4%; Average loss: 0.3060\n",
            "Iteration: 403; Percent complete: 80.6%; Average loss: 0.3784\n",
            "Iteration: 404; Percent complete: 80.8%; Average loss: 0.3208\n",
            "Iteration: 405; Percent complete: 81.0%; Average loss: 0.3026\n",
            "Iteration: 406; Percent complete: 81.2%; Average loss: 0.4003\n",
            "Iteration: 407; Percent complete: 81.4%; Average loss: 0.3190\n",
            "Iteration: 408; Percent complete: 81.6%; Average loss: 0.3096\n",
            "Iteration: 409; Percent complete: 81.8%; Average loss: 0.4099\n",
            "Iteration: 410; Percent complete: 82.0%; Average loss: 0.3859\n",
            "Iteration: 411; Percent complete: 82.2%; Average loss: 0.3562\n",
            "Iteration: 412; Percent complete: 82.4%; Average loss: 0.3380\n",
            "Iteration: 413; Percent complete: 82.6%; Average loss: 0.1897\n",
            "Iteration: 414; Percent complete: 82.8%; Average loss: 0.3714\n",
            "Iteration: 415; Percent complete: 83.0%; Average loss: 0.3510\n",
            "Iteration: 416; Percent complete: 83.2%; Average loss: 0.3345\n",
            "Iteration: 417; Percent complete: 83.4%; Average loss: 0.3436\n",
            "Iteration: 418; Percent complete: 83.6%; Average loss: 0.3833\n",
            "Iteration: 419; Percent complete: 83.8%; Average loss: 0.3658\n",
            "Iteration: 420; Percent complete: 84.0%; Average loss: 0.3687\n",
            "Iteration: 421; Percent complete: 84.2%; Average loss: 0.3263\n",
            "Iteration: 422; Percent complete: 84.4%; Average loss: 0.2731\n",
            "Iteration: 423; Percent complete: 84.6%; Average loss: 0.3306\n",
            "Iteration: 424; Percent complete: 84.8%; Average loss: 0.2658\n",
            "Iteration: 425; Percent complete: 85.0%; Average loss: 0.3765\n",
            "Iteration: 426; Percent complete: 85.2%; Average loss: 0.3743\n",
            "Iteration: 427; Percent complete: 85.4%; Average loss: 0.3751\n",
            "Iteration: 428; Percent complete: 85.6%; Average loss: 0.2960\n",
            "Iteration: 429; Percent complete: 85.8%; Average loss: 0.2946\n",
            "Iteration: 430; Percent complete: 86.0%; Average loss: 0.3953\n",
            "Iteration: 431; Percent complete: 86.2%; Average loss: 0.3506\n",
            "Iteration: 432; Percent complete: 86.4%; Average loss: 0.3497\n",
            "Iteration: 433; Percent complete: 86.6%; Average loss: 0.3517\n",
            "Iteration: 434; Percent complete: 86.8%; Average loss: 0.3985\n",
            "Iteration: 435; Percent complete: 87.0%; Average loss: 0.3273\n",
            "Iteration: 436; Percent complete: 87.2%; Average loss: 0.3327\n",
            "Iteration: 437; Percent complete: 87.4%; Average loss: 0.3149\n",
            "Iteration: 438; Percent complete: 87.6%; Average loss: 0.2893\n",
            "Iteration: 439; Percent complete: 87.8%; Average loss: 0.3567\n",
            "Iteration: 440; Percent complete: 88.0%; Average loss: 0.3159\n",
            "Iteration: 441; Percent complete: 88.2%; Average loss: 0.3824\n",
            "Iteration: 442; Percent complete: 88.4%; Average loss: 0.3381\n",
            "Iteration: 443; Percent complete: 88.6%; Average loss: 0.3583\n",
            "Iteration: 444; Percent complete: 88.8%; Average loss: 0.4314\n",
            "Iteration: 445; Percent complete: 89.0%; Average loss: 0.3563\n",
            "Iteration: 446; Percent complete: 89.2%; Average loss: 0.3537\n",
            "Iteration: 447; Percent complete: 89.4%; Average loss: 0.3620\n",
            "Iteration: 448; Percent complete: 89.6%; Average loss: 0.3726\n",
            "Iteration: 449; Percent complete: 89.8%; Average loss: 0.3771\n",
            "Iteration: 450; Percent complete: 90.0%; Average loss: 0.3169\n",
            "Iteration: 451; Percent complete: 90.2%; Average loss: 0.3562\n",
            "Iteration: 452; Percent complete: 90.4%; Average loss: 0.3673\n",
            "Iteration: 453; Percent complete: 90.6%; Average loss: 0.3483\n",
            "Iteration: 454; Percent complete: 90.8%; Average loss: 0.3412\n",
            "Iteration: 455; Percent complete: 91.0%; Average loss: 0.2840\n",
            "Iteration: 456; Percent complete: 91.2%; Average loss: 0.3742\n",
            "Iteration: 457; Percent complete: 91.4%; Average loss: 0.3816\n",
            "Iteration: 458; Percent complete: 91.6%; Average loss: 0.3458\n",
            "Iteration: 459; Percent complete: 91.8%; Average loss: 0.2690\n",
            "Iteration: 460; Percent complete: 92.0%; Average loss: 0.3448\n",
            "Iteration: 461; Percent complete: 92.2%; Average loss: 0.3314\n",
            "Iteration: 462; Percent complete: 92.4%; Average loss: 0.3300\n",
            "Iteration: 463; Percent complete: 92.6%; Average loss: 0.3078\n",
            "Iteration: 464; Percent complete: 92.8%; Average loss: 0.3608\n",
            "Iteration: 465; Percent complete: 93.0%; Average loss: 0.3415\n",
            "Iteration: 466; Percent complete: 93.2%; Average loss: 0.3178\n",
            "Iteration: 467; Percent complete: 93.4%; Average loss: 0.3347\n",
            "Iteration: 468; Percent complete: 93.6%; Average loss: 0.3469\n",
            "Iteration: 469; Percent complete: 93.8%; Average loss: 0.3544\n",
            "Iteration: 470; Percent complete: 94.0%; Average loss: 0.3443\n",
            "Iteration: 471; Percent complete: 94.2%; Average loss: 0.3057\n",
            "Iteration: 472; Percent complete: 94.4%; Average loss: 0.3557\n",
            "Iteration: 473; Percent complete: 94.6%; Average loss: 0.3251\n",
            "Iteration: 474; Percent complete: 94.8%; Average loss: 0.3219\n",
            "Iteration: 475; Percent complete: 95.0%; Average loss: 0.3173\n",
            "Iteration: 476; Percent complete: 95.2%; Average loss: 0.3333\n",
            "Iteration: 477; Percent complete: 95.4%; Average loss: 0.3356\n",
            "Iteration: 478; Percent complete: 95.6%; Average loss: 0.2940\n",
            "Iteration: 479; Percent complete: 95.8%; Average loss: 0.2869\n",
            "Iteration: 480; Percent complete: 96.0%; Average loss: 0.3565\n",
            "Iteration: 481; Percent complete: 96.2%; Average loss: 0.2692\n",
            "Iteration: 482; Percent complete: 96.4%; Average loss: 0.3582\n",
            "Iteration: 483; Percent complete: 96.6%; Average loss: 0.3034\n",
            "Iteration: 484; Percent complete: 96.8%; Average loss: 0.3543\n",
            "Iteration: 485; Percent complete: 97.0%; Average loss: 0.3666\n",
            "Iteration: 486; Percent complete: 97.2%; Average loss: 0.3796\n",
            "Iteration: 487; Percent complete: 97.4%; Average loss: 0.3504\n",
            "Iteration: 488; Percent complete: 97.6%; Average loss: 0.3706\n",
            "Iteration: 489; Percent complete: 97.8%; Average loss: 0.3271\n",
            "Iteration: 490; Percent complete: 98.0%; Average loss: 0.3121\n",
            "Iteration: 491; Percent complete: 98.2%; Average loss: 0.3471\n",
            "Iteration: 492; Percent complete: 98.4%; Average loss: 0.3182\n",
            "Iteration: 493; Percent complete: 98.6%; Average loss: 0.4371\n",
            "Iteration: 494; Percent complete: 98.8%; Average loss: 0.3658\n",
            "Iteration: 495; Percent complete: 99.0%; Average loss: 0.2933\n",
            "Iteration: 496; Percent complete: 99.2%; Average loss: 0.3094\n",
            "Iteration: 497; Percent complete: 99.4%; Average loss: 0.3000\n",
            "Iteration: 498; Percent complete: 99.6%; Average loss: 0.3116\n",
            "Iteration: 499; Percent complete: 99.8%; Average loss: 0.3517\n",
            "Iteration: 500; Percent complete: 100.0%; Average loss: 0.3266\n"
          ]
        }
      ]
    }
  ]
}